# 📚 Methodologies

> **Note**: Use the TOC provided by the GitHub MD file rendering named `Outline`.

🏡 [Back to home](README.md).

## Methodology personal to assess a web app in whitebox mode

**Flow:**

```mermaid
flowchart LR
    A((Anonymous)) -->B(Authenticated with low privileges)
    B --> C(Authenticated with high privileges)
    C --> D(System read and write access)
    D --> E((System shell))
```

**Step 1:**

🔎 Identify in code all features accessible anonymously and confirm access via an HTTP call.

**Step 2:**

🔎 Search into elements identified in step 1 for vulnerability allowing to pass in an authenticated state (normal or privileged):

* In case of SQLI: Verify if stack queries are supported, if NOT verify the presence of a reset password feature or any feature allowing to retrieve/obtain some kind of access token...
* In case of serialization: Verify if control is performed on the data received (signature, HMAC...)...
* Take care to logic applied in order to identify any issue in it (ex: custom algo or predictable generated elements like reset token...).
* ...

**Step 3:**

🔎 Search for a way (doc, install file, hard-coded, enumeration...) to obtain a user or a list of users in order to leverage the step 2.

**Step 4:**

📖 Use element gathered in steps 1+2+3 to obtain an authenticated session.

📍 **Needed if required for the next step:** If the access was not privileged then repeat step 2+3 targeting feature accessible to the authenticated user in order to pass in privileged mode.

**Step 5:**

📖 Use element gathered in **step 4** to look for a way to interact with the OS filesystem directly (ex: XXE) or via a relay like the DB (ex: custom extension).

**Step 6:**

📖 Use element gathered in **step 5** to look for vulnerability to gain RCE and then a reverse shell.

## Methodology for SAML based authentication flow evaluation

### Documentation

* [Specifications bundle](https://docs.oasis-open.org/security/saml/v2.0/saml-2.0-os.zip).
  * [Glossary](https://docs.oasis-open.org/security/saml/v2.0/saml-glossary-2.0-os.pdf).
  * [Security and Privacy Considerations](https://docs.oasis-open.org/security/saml/v2.0/saml-sec-consider-2.0-os.pdf).
  * [Assertions and Protocols](https://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf).
  * [Authentication Context](https://docs.oasis-open.org/security/saml/v2.0/saml-authn-context-2.0-os.pdf).
  * [Bindings](https://docs.oasis-open.org/security/saml/v2.0/saml-bindings-2.0-os.pdf).
  * [Metadata](https://docs.oasis-open.org/security/saml/v2.0/saml-metadata-2.0-os.pdf).
  * [Profiles](https://docs.oasis-open.org/security/saml/v2.0/saml-profiles-2.0-os.pdf).
  * [Conformance](https://docs.oasis-open.org/security/saml/v2.0/saml-conformance-2.0-os.pdf).

### Methodology: How to Hunt Bugs in SAML?

* [Part I](https://epi052.gitlab.io/notes-to-self/blog/2019-03-07-how-to-test-saml-a-methodology/).
* [Part II](https://epi052.gitlab.io/notes-to-self/blog/2019-03-13-how-to-test-saml-a-methodology-part-two).
* [Part III](https://epi052.gitlab.io/notes-to-self/blog/2019-03-16-how-to-test-saml-a-methodology-part-three).

### Tools

* [Custom python utility script](../templates/inspect_saml_request_response.py).
* Burp extension named  [SAML Raider](https://portswigger.net/bappstore/c61cfa893bb14db4b01775554f7b802e).
* [SAML online toolkit](https://www.samltool.com/online_tools.php).
* [SAML online validator](https://samltool.io/).
* [SAML decoder and parser](https://www.scottbrady91.com/tools/saml-parser).
* [SAML developer tools](https://developer.pingidentity.com/en/tools.html).

### Different types of XML signatures

💡 Based on this [source](https://epi052.gitlab.io/notes-to-self/blog/2019-03-07-how-to-test-saml-a-methodology/#xml-signatures):

* A **enveloping signature** is when the signature wraps the signed resource.
* A **enveloped signature** is when the signature itself is a descendant of the resource it’s signing.
* A **detached signature** is neither wrapping nor is it wrapped by the resource to be signed. Instead, it is wholly separate from the signed resource.

## Methodology for code review

> 🤝 Based on the video, named [Code Review Strategies](https://pentesterlab.com/videos/280), from the [PentesterLab Code Review badge of PentesterLab](https://pentesterlab.com/badges/codereview).

![CodeReviewStrategies](CodeReviewStrategies.png)

🤔 Possible strategies:

1. **Weakness** identification orientation (horizontal approach):
    * Review the code of *all functions even if they are not called by a function using user input*.
    * ✅ This provides a pretty good coverage.
    * ❌ It is time consuming so require to have the time to do it.
2. **Vulnerability** identification orientation (vertical top-down or down-top approach):
    * Review the code of *all functions in the call tree initiated by (top-down) or landing on (down-top) a user input*.
    * ✅ This requires less time and allow to focus on some parts of an app.
    * ❌ As the coverage is partial it does not provide a full feedback about the security of the app.
3. **Feature** orientation:
    * It uses the *vulnerability identification orientation in vertical top-down approach* and review all the functions that are part of a targeted features (for example "password reset").
    * 💡 It is the recommended one when possible.

## Methodology for ports scanning

> 🤝 All credit for this **methodology** go fully to **[Moses Frost](https://linktr.ee/mosesrenegade)**.

📍 Foundation fact:

1. [Masscan](https://github.com/robertdavidgraham/masscan) is ideal for **speed** over **accuracy**.
2. [Nmap](https://github.com/nmap/nmap) is ideal for **accuracy** over **speed**.

📖 The methodology is composed of the following phases:

```mermaid
flowchart LR
    A[(List of hosts)]
    A -- Seed --> B(Masscan)
    B -- Produce --> C[(List of open ports<br/>by hosts)]
    C -- Seed --> D(Nmap)
    D -- Produce --> E[(List of open services<br/>by hosts)]
```

💡 Key points:

* Always scan all **TCP** port range `0-65535`.
* **Never scan UDP in the same way than TCP**: Always use tools with valid **UDP payloads**.
* Consider dividing the ports range like this:
  * `0-1024`: Default server ports.
  * `1025-10000`: Commonly used server ports.
  * `30000-32767`: Kubernetes Nodes NodePorts services.
  * `10001-29999` and `32768-65535`: Customizable by the end customers.

## Methodology for Cloud penetration testing

> 👨‍💻 Contains all information that I have gathered/created during my studies/work/training on this topic.

> 🤝 All credit for this **methodology** as well as the **Assets Collection Pipeline** concepts go fully to **[Moses Frost](https://linktr.ee/mosesrenegade)**.

📖 The methodology is composed of the following phases:

```mermaid
flowchart TD
    A[Reconnaissance/Data Collection] --> B(Data Analysis for Discovery)
    B --> C(Scanning/Mapping)
    C --> D(Vulnerability/Discovery)
    D --> E(Exploitation/Elevation)
    E -->F(Post Exploitation Reconnaissance)
    F--> G(Persistence/Pivot Pivoting)
    G--> H[Lessons Learned/Remediation]
```

🏭 The **Assets Collection Pipeline** flow is the following:

```mermaid
flowchart LR
    subgraph Seed Value
        A[Domains]
    end
    subgraph Hosts
        A --> B(Subdomains/Hostnames)
        A --> C("Hosts (IP)")
    end
    subgraph Applications
        B --> D(Routes/Endpoints)
        B --> E(Ports)
        C --> E
    end
    subgraph Classifier   
        E --> F(Technology)
        D --> F
    end
    subgraph Detection
        F --> G[Issues]
    end
```

🏭 My implementation of the **Assets Collection Pipeline** is composed of the following phases:

```mermaid
sequenceDiagram
    actor DOM as righettod
    participant CAD as Cloud Assets Discovery
    participant ADS as Attack Surface Discovery
    participant ADA as Attack Surface Analysis
    DOM->>CAD: Provide the target's base domain<br/>like "righettod.eu"
    CAD->>CAD: Use different open data providers<br/>and techniques to find exposed assets
    CAD->>CAD: Produce a CSV with identified assets
    CAD->>ADS: Pass the relay
    ADS->>ADS: Consume the CSV data
    ADS->>ADS: Scan each asset for exposed services
    ADS->>ADS: Produce a CSV with identified 
    ADS->>ADA: Pass the relay
    ADA->>ADA: Consume the CSV data
    ADA->>ADA: Identify interesting services<br/>using different custom rules
    ADA->>ADA: Produce a CSV with selected services
    ADA->>DOM: Pass the relay
    DOM->>DOM: Consume the CSV data
    DOM->>DOM: Analyze each service for<br/>presence of vulnerabilities
    DOM-->>DOM: Identify, document and exploit<br/>any vulnerability according<br/>to the provided scope
```

📦 My **Assets Collection Pipeline** implementation's toolkit is composed of the following items:

* 📑 **Cloud Assets Discovery** phase is handled by this [custom scripts](../scripts/discover-cloud-based-assets.sh).
* 📑 **Attack Surface Discovery** phase is handled by this [custom scripts](../scripts/attack-cloud-based-assets.sh).
* 🚧 **Attack Surface Analysis** phase tooling creation is pending...

💡 For the **Attack Surface Discovery** phase, the tools [GoWitness](https://github.com/sensepost/gowitness) should be used to obtain a screenshot of every website identified via the capture of the *welcome page* for each of them.

💻 The following command can be used - Docker version is used to prevent any issues with the dependencies of GoWitness and ensure to always use the latest stable release:

```powershell
PS> get-content .\urls.txt
https://righettod.eu
https://lessentiel.lu
PS> docker run --rm -v "$pwd/:/data" leonjza/gowitness gowitness file -f urls.txt
# Processing ...
PS> get-childItem .\screenshots\
https-righettod.eu.png
https-lessentiel.lu.png
```
