"""
Script to find, as much as possible, subdomains of a company pointing to a cloud service 
provider like HEROKU/RENDER/etc. via a CNAME DNS records on the company DNS domain records.
The objective is to identify any subdomain potentially prone to a subdomain hijacking 
due to the fact that the app/account on the cloud service provider was removed BUT NOT the 
associated CNAME records.

Dependency:
    pip install requests termcolor
 
Remark:
    Focus was made on reliability of data retrieved over the performance aspects.
    
    Python was used to easily process "CRT.sh" large JSON data.
    
    Direct SQL access to "CRT.sh" was not used (after several tests) due to
    the small timeout set on query even after several query tunning operations.

CRT.sh query:
    https://crt.sh/?Identity=[BASE_DOMAIN]&exclude=expired&output=json
"""
import requests
import argparse
import re
import time
from termcolor import colored

# Constants
PROVIDERS_SUBDOMAINS_SELECTED = ["herokuapp.com", "onrender.com", "github.io", "workers.dev", "qualtrics.com", "akamaized.net", "highq.com", "ceros.com"]
PROVIDERS_SUBDOMAINS_REGEX = f"^.*({'|'.join(PROVIDERS_SUBDOMAINS_SELECTED)})$"
CSV_FILE_HEADERS = "Provider,ProviderSubdomain,CompanySubdomain"
CRT_QUERY_TEMPLATE = "https://crt.sh/?Identity=%s&exclude=expired&output=json"
CRT_QUERY_WAIT_DELAY_IN_SECONDS = 1.5


def extract_crt_records(base_domain):
    response_code = 0
    data = None
    target_url = CRT_QUERY_TEMPLATE % base_domain
    while response_code != 200:
        resp = requests.get(target_url)
        response_code = resp.status_code
        if response_code == 200:
            data = resp.json()
        else:
            time.sleep(CRT_QUERY_WAIT_DELAY_IN_SECONDS)
    return data


def process_crt_records(crt_json):
    domains = []
    for ct_entry in crt_json:
        common_name = ct_entry["common_name"]
        if (len(re.findall(PROVIDERS_SUBDOMAINS_REGEX, common_name.strip(" "), re.IGNORECASE))) > 0:
            provider = common_name.split(".")[-2].upper()
            for name_value in ct_entry["name_value"].split("\n"):
                domains.append((provider, common_name, name_value))
    return domains


def generate_csv_file(domains, csv_file_path):
    lines = []
    for domain in domains:
        line = f'"{domain[0]}","{domain[1]}","{domain[2]}"'
        if line not in lines:
            lines.append(line)
    lines.sort()
    with open(csv_file_path, mode="w", encoding="utf-8") as f:
        f.write(f"{CSV_FILE_HEADERS}\n")
        f.write("\n".join(lines))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Script to find, as much as possible, subdomains of a company pointing to a cloud service provider like HEROKU/RENDER/etc.")
    required_params = parser.add_argument_group("required named arguments")
    required_params.add_argument("-b", action="store", dest="base_domain", help="Target company base domain (ex: 'righettod.eu').", required=True)
    parser.add_argument("-o", action="store", dest="output_csv_file_path", help="Full path to the CSV output file (default to 'subdomains.csv').", required=False, default="subdomains.csv")
    args = parser.parse_args()
    base_domain = args.base_domain
    csv_file_path = args.output_csv_file_path
    print(colored(f"[+] Extract Certificate Transparency records (via 'crt.sh') for the base domain '{base_domain}' ...", "yellow"))
    ct_records = extract_crt_records(base_domain)
    print(f"{len(ct_records)} records retrieved.")
    print(colored(f"[+] Anazlyse retrieved records ...", "yellow"))
    domains = process_crt_records(ct_records)
    count_domains = (len(domains))
    if count_domains > 0:
        print(f"{len(domains)} subdomains identified.")
        print(colored(f"[+] Generate the CSV file ...", "yellow"))
        generate_csv_file(domains, csv_file_path)
        print(colored(f"[V] File '{csv_file_path}' generated.", "green"))
    else:
        print(colored(f"[!] No subdomains identified.", "red"))
